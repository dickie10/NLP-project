{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "177dea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose from large or small or custom named dataset\n",
    "\n",
    "## for custom named dataset, the MLP parameters for small dataset are used\n",
    "\n",
    "file = \"small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score as precision, recall_score as recall\n",
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc58ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ce13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:label for i, label in enumerate(labels)}\n",
    "label2id = {label:i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e2ebeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_sparse_csr(f\"train_{file}\")\n",
    "test = load_sparse_csr(f\"test_{file}\")\n",
    "retrieved = load_sparse_csr(f\"test_retrieved_{file}\")\n",
    "ytrain = pd.read_json(f\"train_label_{file}.jsonl\", lines = True)\n",
    "ytest = pd.read_json(f\"test_label_{file}.jsonl\", lines = True)\n",
    "yretrieved = pd.read_json(f\"neg_label_{file}.jsonl\", lines = True)\n",
    "yret = yretrieved[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4cd9b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 32179\n",
      "Test len: 16742\n",
      "Retrieved Test len: 36185\n"
     ]
    }
   ],
   "source": [
    "print(\"Train len: \" + str(len(ytrain)))\n",
    "print(\"Test len: \" + str(len(ytest)))\n",
    "print(\"Retrieved Test len: \" + str(len(yret)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed421b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, xtest, ytest, log_string):\n",
    "    print(f\"Performance for {log_string}:\")\n",
    "    y_pred = model.predict(xtest)\n",
    "    \n",
    "    yt = [id2label[y] for y in ytest]\n",
    "    yp = [id2label[y] for y in y_pred]\n",
    "    \n",
    "    f1_average = f1_score(y_true=ytest, y_pred = y_pred, average='weighted')\n",
    "    p = precision(ytest, y_pred, average = 'weighted')\n",
    "    r = recall(ytest, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_average,\n",
    "               'precision': p,\n",
    "               'recall': r,\n",
    "               'accuracy': accuracy}\n",
    "    print(metrics)\n",
    "    \n",
    "    cm = confusion_matrix(yt, yp, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    seaborn.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Reds')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf89e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier\n",
    "mlp = 0\n",
    "if file == \"large\":\n",
    "    mlp = MLP(random_state=101, activation='relu', solver=\"adam\", hidden_layer_sizes=(40,40), max_iter=200,\n",
    "          batch_size=1024, verbose=True, early_stopping=True)\n",
    "else:\n",
    "    mlp = MLP(random_state=101, activation='relu', solver=\"adam\", hidden_layer_sizes=(40), max_iter=200,\n",
    "          batch_size=256, verbose=True, early_stopping=True)\n",
    "mlp.fit(train, ytrain[\"label\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.plot(mlp.loss_curve_, label='Training Loss', color='blue')\n",
    "plt.plot(mlp.validation_scores_, label='Validation Score', color='red')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss/ValidationScore')\n",
    "plt.title('Training and Validation Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245d23c",
   "metadata": {},
   "source": [
    "### Predictions with true evidences given in dataset (Type-I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(mlp, test, ytest[\"label\"].to_numpy(), \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72fcd2",
   "metadata": {},
   "source": [
    "### Predictions with retrieved Evidences (Type-II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7694c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(mlp, retrieved, yret, \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2059d40",
   "metadata": {},
   "source": [
    "### Were wrong evidences discarded? (Robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yneg = yretrieved[yretrieved.neg_label == 0][\"neg_label\"]\n",
    "yneg[:] = 2\n",
    "get_predictions(mlp, retrieved[yretrieved.neg_label.to_numpy() == 0], yneg, \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1831105",
   "metadata": {},
   "source": [
    "### Performance with correct evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e7675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypos = yretrieved[yretrieved.neg_label == 1][\"label\"]\n",
    "get_predictions(mlp, retrieved[yretrieved.neg_label.to_numpy() == 1], ypos, \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1adeb",
   "metadata": {},
   "source": [
    "### Performance with incorrect evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yneg = yretrieved[yretrieved.neg_label == 0][\"label\"]\n",
    "get_predictions(mlp, retrieved[yretrieved.neg_label.to_numpy() == 0], yneg, \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723ce7f",
   "metadata": {},
   "source": [
    "### FEVER Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFEVERScore(model, test, y, y_neg):\n",
    "    y_pred = model.predict(test)\n",
    "    true = 0\n",
    "    for i, element in enumerate(y_neg):\n",
    "        if(element == 1):\n",
    "            if(y_pred[i] == y[i]):\n",
    "                true += 1\n",
    "    \n",
    "    return true/len(y_pred)\n",
    "\n",
    "yneg = yretrieved.neg_label.to_numpy()\n",
    "fever = getFEVERScore(mlp, retrieved, yret, yneg)\n",
    "print(f'FEVER Score: {fever}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da9456",
   "metadata": {},
   "source": [
    "### Performance on Refuted Samples of true calim-evidence pairs of Type-II Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bab561",
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy = recall in this case\n",
    "\n",
    "y = yretrieved[(yretrieved.label == 1) & (yretrieved.neg_label == 1)][\"label\"]\n",
    "get_predictions(mlp, retrieved[(yretrieved.label == 1) & (yretrieved.neg_label == 1)], y.to_numpy(), \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389193d",
   "metadata": {},
   "source": [
    "### Recall of 'REFUTES' for corret+incorrect pairs in Type-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = recall\n",
    "\n",
    "y = yretrieved[yretrieved.label == 1][\"label\"]\n",
    "get_predictions(mlp, retrieved[yretrieved.label == 1], y.to_numpy(), \"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68785a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
